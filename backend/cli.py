# * CLI version of IntelliSearch
import os
import dotenv
import json
import logging
import asyncio
import argparse
import sys


sys.path.append(os.getcwd())
# set up logging config
from utils.log_config import setup_logging

logging.getLogger("mcp").setLevel(logging.CRITICAL)


from pathlib import Path
from openai import OpenAI
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from zai import ZhipuAiClient
from mcp_module.server_manager import MultiServerManager
from mcp.types import CallToolResult
from colorama import Fore, Style, init
from prompt_toolkit import prompt
from rich.console import Console
from backend.tool_hash import fix_tool_args


init(autoreset=True)
dotenv.load_dotenv(override=True)
setup_logging(log_file_path="./log/mcp.log", project_prefix="IntelliSearch Main")


class MCPChat:
    def __init__(
        self,
        model_name: str = "deepseek-chat",
        system_prompt: str = "You are a helpful assistant",
        server_config_path: str = "./config.json",
        max_tool_call: int = 5,
    ):
        self.model_name = model_name
        self.history = []
        self.system_prompt = system_prompt
        self.history.append({"role": "system", "content": system_prompt})
        self.time_stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.result_dir = "./results"
        self.max_tool_call = int(max_tool_call)
        os.makedirs(self.result_dir, exist_ok=True)

        self.base_url = os.environ.get("BASE_URL")
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "Environment variable 'OPENAI_API_KEY' not found. Please set it."
            )
        
        self.client = OpenAI(api_key=api_key, base_url=self.base_url)

        # handle mcp settings and connecting mcp servers
        self.config_path = server_config_path
        self.config = self.load_server_configs(config_path=self.config_path)
        self.server_manager = MultiServerManager(server_configs=self.config)
        # list all the tools available
        self.available_tools = []

        # setup logger
        self.logger = logging.getLogger(__name__)
        TOOL_CALL_ERROR = 35
        logging.addLevelName(TOOL_CALL_ERROR, "TOOL CALL ERROR")
        self.logger.info(
            f"DeepSeek Chat with MCP enhancement client initialized for model: {self.model_name}"
        )

    def stream_chat_response(self, available_tools):
        """
        ÊâßË°åÊµÅÂºèÂìçÂ∫îÈÄªËæëÔºàÂêåÊ≠•ÁâàÊú¨ÔºåÂÖºÂÆπ DeepSeek / OpenAI SDKÔºâ
        """
        result_text = ""
        if available_tools:
            with self.client.chat.completions.stream(
                model=self.model_name,
                messages=self.history,
                tools=available_tools,
            ) as stream:
                for event in stream:
                    if hasattr(event, "chunk") and event.chunk.choices:
                        delta = event.chunk.choices[0].delta

                        if getattr(delta, "content", None):
                            print(
                                Style.BRIGHT + Fore.YELLOW + delta.content,
                                end="",
                                flush=True,
                            )
                            result_text += delta.content

                        if getattr(delta, "tool_calls", None):
                            for tool in delta.tool_calls:
                                func = getattr(tool, "function", None)
                                if func:
                                    if func.name:
                                        print(
                                            Fore.GREEN + f"\nüîß Tool name: {func.name}"
                                        )
                                    if func.arguments:
                                        print(
                                            Fore.GREEN + func.arguments,
                                            end="",
                                            flush=True,
                                        )

                final_message = stream.get_final_completion()
                return result_text, final_message
        else:
            # no tools for streaming response
            with self.client.chat.completions.stream(
                model=self.model_name,
                messages=self.history,
            ) as stream:
                for event in stream:
                    if hasattr(event, "chunk") and event.chunk.choices:
                        delta = event.chunk.choices[0].delta

                        if getattr(delta, "content", None):
                            print(
                                Style.BRIGHT + Fore.YELLOW + delta.content,
                                end="",
                                flush=True,
                            )
                            result_text += delta.content

                        if getattr(delta, "tool_calls", None):
                            for tool in delta.tool_calls:
                                func = getattr(tool, "function", None)
                                if func:
                                    if func.name:
                                        print(
                                            Fore.GREEN + f"\nüîß Tool name: {func.name}"
                                        )
                                    if func.arguments:
                                        print(
                                            Fore.GREEN + func.arguments,
                                            end="",
                                            flush=True,
                                        )

                final_message = stream.get_final_completion()
                return result_text, final_message

    async def chat_completion_stream(
        self, user_message: str
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ÁúüÊ≠£ÁöÑÂºÇÊ≠•ÊµÅÂºèÂ§ÑÁêÜÂáΩÊï∞Ôºåyield ÊØè‰∏Ä‰∏™Êù•Ëá™ OpenAI SDK ÁöÑ delta
        ÊîØÊåÅÂ§öËΩÆÂ∑•ÂÖ∑Ë∞ÉÁî®

        Yields:
            Dict[str, Any]: ÂåÖÂê´‰ª•‰∏ãÂèØËÉΩÁöÑÈîÆ:
                - type: "content" (ÊñáÊú¨ÂÜÖÂÆπ) Êàñ "tool_call" (Â∑•ÂÖ∑Ë∞ÉÁî®) Êàñ "tool_result" (Â∑•ÂÖ∑ÁªìÊûú)
                - content: ÊñáÊú¨ÂÜÖÂÆπ (ÂΩì type ‰∏∫ "content" Êó∂)
                - tool_call: Â∑•ÂÖ∑Ë∞ÉÁî®‰ø°ÊÅØ (ÂΩì type ‰∏∫ "tool_call" Êó∂)
                - tool_result: Â∑•ÂÖ∑ÊâßË°åÁªìÊûú (ÂΩì type ‰∏∫ "tool_result" Êó∂)
        """
        try:
            # Ëé∑ÂèñÂèØÁî®Â∑•ÂÖ∑
            tools = await self.list_tools()
            self.logger.info(f"Available Tools: {tools}")
            self.tools_store = tools
            available_tools = [
                {
                    "type": "function",
                    "function": {
                        "name": f"{tool.get('name')}",
                        "description": tool.get("description"),
                        "input_schema": tool.get("input_schema"),
                    },
                }
                for tool in list(tools.values())
            ]

            # Ê∑ªÂä†Áî®Êà∑Ê∂àÊÅØÂà∞ÂéÜÂè≤ËÆ∞ÂΩï
            self.history.append({"role": "user", "content": user_message})

            # Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®Âæ™ÁéØ
            for round_count in range(self.max_tool_call):
                self.logger.info(f"Calling tool response for round: {round_count + 1}")

                # ÂàõÂª∫ÊµÅÂºèÂìçÂ∫î
                if available_tools:
                    with self.client.chat.completions.create(
                        model=self.model_name,
                        messages=self.history,
                        tools=available_tools,
                        stream=True,
                    ) as stream:
                        # Êî∂ÈõÜÂ∑•ÂÖ∑Ë∞ÉÁî®
                        tool_calls = []
                        current_tool_call = None

                        # Â§ÑÁêÜÊµÅÂºèÂìçÂ∫î
                        for chunk in stream:
                            if chunk.choices and chunk.choices[0].delta:
                                delta = chunk.choices[0].delta

                                # Â§ÑÁêÜÂÜÖÂÆπ
                                if getattr(delta, "content", None):
                                    yield {"type": "content", "content": delta.content}

                                # Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®
                                if getattr(delta, "tool_calls", None):
                                    for tool_delta in delta.tool_calls:
                                        # Â¶ÇÊûúÊòØÊñ∞Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÂºÄÂßã
                                        if tool_delta.index >= len(tool_calls):
                                            tool_calls.append(
                                                {
                                                    "id": tool_delta.id,
                                                    "type": "function",
                                                    "function": {
                                                        "name": (
                                                            tool_delta.function.name
                                                            if tool_delta.function.name
                                                            else ""
                                                        ),
                                                        "arguments": (
                                                            tool_delta.function.arguments
                                                            if tool_delta.function.arguments
                                                            else ""
                                                        ),
                                                    },
                                                }
                                            )
                                            current_tool_call = tool_calls[-1]
                                            # ÂèëÈÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®ÂºÄÂßã‰ø°Âè∑
                                            yield {
                                                "type": "tool_call_start",
                                                "tool_call": {
                                                    "id": tool_delta.id,
                                                    "name": (
                                                        tool_delta.function.name
                                                        if tool_delta.function.name
                                                        else ""
                                                    ),
                                                },
                                            }
                                        else:
                                            # Êõ¥Êñ∞Áé∞ÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®
                                            current_tool_call = tool_calls[
                                                tool_delta.index
                                            ]

                                        # Êõ¥Êñ∞Â∑•ÂÖ∑Ë∞ÉÁî®‰ø°ÊÅØ
                                        if tool_delta.function.name:
                                            current_tool_call["function"][
                                                "name"
                                            ] = tool_delta.function.name
                                        if tool_delta.function.arguments:
                                            current_tool_call["function"][
                                                "arguments"
                                            ] += tool_delta.function.arguments

                                        # ÂèëÈÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®Êõ¥Êñ∞
                                        yield {
                                            "type": "tool_call_delta",
                                            "tool_call": {
                                                "id": current_tool_call["id"],
                                                "name": current_tool_call["function"][
                                                    "name"
                                                ],
                                                "arguments": (
                                                    tool_delta.function.arguments
                                                    if tool_delta.function.arguments
                                                    else ""
                                                ),
                                            },
                                        }
                else:
                    # Ê≤°ÊúâÂ∑•ÂÖ∑ÁöÑÊµÅÂºèÂìçÂ∫î
                    with self.client.chat.completions.create(
                        model=self.model_name, messages=self.history, stream=True
                    ) as stream:
                        # Â§ÑÁêÜÊµÅÂºèÂìçÂ∫î
                        for chunk in stream:
                            if chunk.choices and chunk.choices[0].delta:
                                delta = chunk.choices[0].delta
                                if getattr(delta, "content", None):
                                    yield {"type": "content", "content": delta.content}

                # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÂ∑•ÂÖ∑Ë∞ÉÁî®
                if tool_calls:
                    # Ê∑ªÂä†Âä©ÊâãÊ∂àÊÅØÂà∞ÂéÜÂè≤ËÆ∞ÂΩï
                    assistant_message = {"role": "assistant", "tool_calls": tool_calls}
                    self.history.append(assistant_message)

                    # Â§ÑÁêÜÊØè‰∏™Â∑•ÂÖ∑Ë∞ÉÁî®
                    tool_results_for_history = []
                    for tool_call in tool_calls:
                        tool_name = tool_call["function"]["name"]
                        print("\n" + f"üöÄCalling Tools {tool_name}" + "\n")
                        self.logger.info(f"Calling tools: {tool_call}")

                        tool_name_long = None
                        for chunk in list(tools.values()):
                            if chunk.get("name") == tool_name:
                                tool_name_long = (
                                    f"{chunk.get('server')}:{chunk.get('name')}"
                                )
                                break

                        if not tool_name_long:
                            tool_result_content = (
                                f"Error: Tool '{tool_name}' not found."
                            )
                        else:
                            try:
                                tool_args = json.loads(
                                    tool_call["function"]["arguments"]
                                )
                                self.logger.info(f"Tool Calling args: {tool_args}")
                                # ‰øÆÂ§çÂ∑•ÂÖ∑ÂèÇÊï∞
                                tool_args = fix_tool_args(
                                    tools=tools,
                                    tool_args=tool_args,
                                    tool_name=tool_name_long,
                                )
                                result: CallToolResult = await self.get_tool_response(
                                    call_params=tool_args, tool_name=tool_name_long
                                )
                                tool_result_content = result.model_dump()["content"][0][
                                    "text"
                                ]
                            except Exception as tool_e:
                                tool_result_content = f"Tool execution failed: {tool_e}"

                        # Ê∑ªÂä†Â∑•ÂÖ∑ÁªìÊûúÂà∞ÂéÜÂè≤ËÆ∞ÂΩï
                        tool_result_entry = {
                            "role": "tool",
                            "content": tool_result_content,
                            "tool_call_id": tool_call["id"],
                        }
                        tool_results_for_history.append(tool_result_entry)

                        # ÂèëÈÄÅÂ∑•ÂÖ∑ÁªìÊûú
                        yield {
                            "type": "tool_result",
                            "tool_result": {
                                "id": tool_call["id"],
                                "name": tool_name,
                                "result": tool_result_content,
                            },
                        }

                    # Êâ©Â±ïÂéÜÂè≤ËÆ∞ÂΩï
                    self.history.extend(tool_results_for_history)
                    continue
                else:
                    # Ê≤°ÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºåÁªìÊùüÂØπËØù
                    return

            # Ë∂ÖËøáÂ∑•ÂÖ∑Ë∞ÉÁî®ÈôêÂà∂
            self.logger.error(f"Tool calling limits for {self.max_tool_call} times")
            self.history.append(
                {
                    "role": "user",
                    "content": "Error, you have reached the maximum limit of tool calling requests. Please use the information you get by the tools to generate the final answer.",
                }
            )

            # ‰∏çÂ∏¶Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÊúÄÁªàÂìçÂ∫î
            with self.client.chat.completions.create(
                model=self.model_name, messages=self.history, stream=True
            ) as stream:
                # Â§ÑÁêÜÊúÄÁªàÂìçÂ∫î
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta:
                        delta = chunk.choices[0].delta
                        if getattr(delta, "content", None):
                            yield {"type": "content", "content": delta.content}

            return

        except Exception as e:
            error_message = f"Error calling LLM API: {e}"
            self.logger.error(error_message, exc_info=True)
            yield {"type": "error", "error": error_message}

    async def process_query_stream(
        self, user_message: str
    ) -> AsyncGenerator[str, None]:
        """
        Â§ÑÁêÜÊü•ËØ¢Âπ∂‰ª•ÊµÅÂºèÊñπÂºèËøîÂõûÂìçÂ∫îÔºåÈÄÇÈÖçFastAPIÁöÑStreamingResponse
        """
        async for event in self.chat_completion_stream(user_message):
            if event["type"] == "content":
                # Áõ¥Êé•yieldÊñáÊú¨ÂÜÖÂÆπ
                yield event["content"]
            elif event["type"] == "error":
                # ÈîôËØØ‰ø°ÊÅØ
                yield f"[LLM Error]: {event['error']}\n"
            # ÂÖ∂‰ªñÁ±ªÂûã‰∫ã‰ª∂(Â∑•ÂÖ∑Ë∞ÉÁî®Á≠â)ÂèØ‰ª•ÈÄâÊã©ÊÄßÂ§ÑÁêÜÊàñÂøΩÁï•

    async def list_tools(self):
        try:
            self.logger.info("üîå Connecting and discovering tools...")
            all_tools = await self.server_manager.connect_all_servers()
            with open(
                f"./results/{self.time_stamp}_list_tools.json", "w", encoding="utf-8"
            ) as file:
                json.dump(all_tools, file, indent=4, ensure_ascii=False)

            if not all_tools:
                raise RuntimeError("No tools discovered.")
            return all_tools
        except Exception as e:
            self.logger.error(f"Error while connecting MCP Servers: {e}")
            return []
        finally:
            await self.server_manager.close_all_connections()

    # async def process

    def load_server_configs(self, config_path: Path):
        """‰ªé MCP config Êñá‰ª∂Âä†ËΩΩÂπ∂ËΩ¨Êç¢ server ÈÖçÁΩÆ"""
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        servers = []

        for name, conf in cfg.get("mcpServers", {}).items():
            if conf.get("transport") == "sse":
                servers.append(
                    {
                        "name": name,
                        "url": conf.get("url"),
                        "transport": conf.get("transport", "sse"),
                    }
                )
            else:
                servers.append(
                    {
                        "name": name,
                        "command": [conf.get("command")] + conf.get("args", []),
                        "env": conf.get("env"),
                        "cwd": conf.get("cwd"),
                        "transport": conf.get("transport", "stdio"),
                        "port": conf.get("port", None),
                        "endpoint": conf.get("endpoint", "/mcp"),
                    }
                )
        return servers

    async def get_tool_response(
        self,
        call_params: Optional[Dict[str, Any]] = None,
        tool_name: Optional[str] = None,
    ) -> Any:
        """ËøûÊé• MCP server Âπ∂Ë∞ÉÁî®Â∑•ÂÖ∑ÔºåËøîÂõûÁªìÊûú"""
        try:
            self.logger.info("üîå Connecting and discovering tools...")
            all_tools = await self.server_manager.connect_all_servers()

            if not all_tools:
                raise RuntimeError("No tools discovered.")

            if tool_name is None:
                tool_name = next(iter(all_tools.keys()))
            if tool_name not in all_tools:
                raise ValueError(f"Tool '{tool_name}' not found.")

            self.logger.info(f"üöÄ Calling tool: {tool_name}")
            result = await self.server_manager.call_tool(
                tool_name, call_params or {}, use_cache=False
            )
            self.logger.info("‚úÖ Tool call SUCCESS.")
            return result

        finally:
            await self.server_manager.close_all_connections()

    def get_system_prompt(self):
        return self.system_prompt

    def append_history(self, append_history: Optional[List[Dict[str, str]]] = None):
        if append_history:
            try:
                for history_episode in append_history:
                    role = history_episode.get("role", None)
                    if role and role in ("system", "user", "assistant"):
                        self.history.append(history_episode)
                    else:
                        self.logger.error(f"Error for role: {role}")
            except Exception as e:
                self.logger.error(f"Updating History Failed: {e}")

    async def process_query(self, user_message: str, stream: bool = True):
        tools = await self.list_tools()
        self.logger.info(f"Available Tools: {tools}")
        self.tools_store = tools
        available_tools = [
            {
                "type": "function",
                "function": {
                    "name": f"{tool.get("name")}",
                    "description": tool.get("description"),
                    "input_schema": tool.get("input_schema"),
                },
            }
            for tool in list(tools.values())
        ]

        # getting self.history
        self.history.append({"role": "user", "content": user_message})

        try:
            for round_count in range(self.max_tool_call):
                # *fix: older version for non-streaming output
                # response = self.client.chat.completions.create(
                #     model=self.model_name,
                #     messages=self.history,
                #     tools=available_tools,
                #     stream=False,
                # )
                self.logger.info(f"Calling tool response for round: {round_count + 1}")
                final_text, response = self.stream_chat_response(
                    available_tools=available_tools
                )

                content = response.choices[0]
                if content.finish_reason == "tool_calls":
                    self.history.append(content.message.model_dump())
                    tool_results_for_history = []
                    for tool_call_single in content.message.tool_calls:

                        # tool_call_single = content.message.tool_calls[0]
                        tool_name = tool_call_single.function.name
                        print("\n" + f"üöÄCalling Tools {tool_name}" + "\n")
                        self.logger.info(f"Calling tools: {tool_call_single}")

                        tool_name_long = None
                        for chunk in list(tools.values()):
                            if chunk.get("name") == tool_name:
                                tool_name_long = (
                                    f"{chunk.get("server")}:{chunk.get("name")}"
                                )
                                break

                        if not tool_name_long:
                            tool_result_content = (
                                f"Error: Tool '{tool_name}' not found."
                            )
                        else:
                            try:
                                tool_args = json.loads(
                                    tool_call_single.function.arguments
                                )
                                self.logger.info(f"Tool Calling args: {tool_args}")
                                # pass add fix for tool names and tool_args
                                tool_args = fix_tool_args(
                                    tools=tools,
                                    tool_args=tool_args,
                                    tool_name=tool_name_long,
                                )
                                result: CallToolResult = await self.get_tool_response(
                                    call_params=tool_args, tool_name=tool_name_long
                                )
                                tool_result_content = result.model_dump()["content"][0][
                                    "text"
                                ]
                            except Exception as tool_e:
                                tool_result_content = f"Tool execution failed: {tool_e}"

                        # add tool result into message lists
                        tool_results_for_history.append(
                            {
                                "role": "tool",
                                "content": tool_result_content,
                                "tool_call_id": tool_call_single.id,
                            }
                        )
                    self.history.extend(tool_results_for_history)
                    continue

                else:
                    # LLM finish calling tools, will return the final response
                    final_content = response.choices[0].message.content
                    self.history.append({"role": "assistant", "content": final_content})
                    return final_content

            # exceed tool calling limit
            self.logger.error(f"Tool calling limits for {self.max_tool_call} times")
            self.history.append(
                {
                    "role": "user",
                    "content": "Error, you have reached the maximum limit of tool calling requests. Please use the information you get by the tools to generate the final answer.",
                }
            )
            # let llm answer the questions without giving the tools
            # response = self.client.chat.completions.create(
            #     model=self.model_name,
            #     messages=self.history,
            #     # no tools given
            #     stream=False,
            # )
            final_text, response = self.stream_chat_response(available_tools=[])

            # get final result
            final_content = response.choices[0].message.content
            self.history.append({"role": "assistant", "content": final_content})
            return final_content

        except Exception as e:
            error_message = f"Error calling LLM API: {e}"
            self.logger.error(error_message, exc_info=True)

            if stream:
                return (chunk for chunk in [f"[LLM Error]: {error_message}\n"])
            else:
                raise RuntimeError(error_message)

    def export_message(self, output_file_path: str = None):
        pass
        if not output_file_path:
            output_file_path = os.path.join(
                self.result_dir, f"./{self.time_stamp}_memory.json"
            )
        dir, file = os.path.split(output_file_path)
        os.makedirs(dir, exist_ok=True)
        with open(output_file_path, "w", encoding="utf-8") as file:
            json.dump(self.history, file, ensure_ascii=False, indent=4)
            self.logger.info(f"Successfully writing messages into {output_file_path}")


async def query(chat_client: MCPChat, user_query):
    # console.print("\n" + "[bold green]Model Response:[/bold green]")
    result = await chat_client.process_query(
        user_message=user_query,
        stream=False,
    )
    chat_client.export_message()
    print("\n\n")
    # console.print("\n\n" + result + "\n\n")


if __name__ == "__main__":
    # SYSTME_PROMPT = "You name is Jiao-Xiao AI (‰∫§Â∞èAI), an intelligent agent launched by the Geek Center of the School of Artificial Intelligence, Shanghai Jiao Tong University. You can use various tools in multi-turn conversations to fulfill user requests, and you are not allowed to use markdown features like bold or italics.\n ATTENTION! ÊØè‰∏ÄËΩÆÂØπËØù‰Ω†Âè™ÂÖÅËÆ∏Ë∞ÉÁî®‰∏ÄËΩÆÂ∑•ÂÖ∑ÔºÅ"

    with open("prompts/base_system_prompt.md", "r", encoding="utf-8") as file:
        SYSTEM_PROMPT = file.read()
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, default="deepseek-chat")
    parser.add_argument("--max_tool_calls", type=int, default=10)
    parser.add_argument("--system_prompt", type=str, default=SYSTEM_PROMPT)
    args = parser.parse_args()

    console = Console()
    chat_client = MCPChat(
        # model_name=args.model_name,
        model_name="glm-4.5",
        system_prompt=args.system_prompt,
        max_tool_call=args.max_tool_calls,
    )
    while True:
        user_input = prompt("Input your qquery: ")
        if str(user_input).lower() == "/exit":
            console.print("[bold red]Exiting... Goodbye! üëã[/bold red]")
            break
        asyncio.run(query(chat_client=chat_client, user_query=user_input))
